Needs checking:
1) Not sure if the arguments for the stft is correct - used the example code to fill in the gaps for now
2) Power-law compression? - as above, not sure about the formula (mu-law??)
3) Double check that input audio are indeed mono (by taking the left channel if stereo)
4) Convolution network - padding valid vs same? ; 'Context' in the table in the paper??
5) LSTM settings; Bidirectional merge_mode
6) Reshape tuple ordering correct? (position of num_speakers)
7) Can spectrograms be combined directly (instead of wav files)? Even after power law compression?
8) Include permutation variations when mixing audio to create the dataset?
9) '{val_loss:.2f}' in ModelCheckpoint path gives KeyError - trouble saving based on metric in general

To be completed:
1) Dependencies list in readme may not be complete - but not too important atm
2) 

